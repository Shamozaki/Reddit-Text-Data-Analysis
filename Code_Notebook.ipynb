{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jlssd-CQOOXE",
        "lyQyR27z48nr",
        "I8E010UbQyML"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Data Analysis on social network Reddit**"
      ],
      "metadata": {
        "id": "t_hYgOgUzIwe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAPWR2s3nzA"
      },
      "source": [
        "# Part 1 - \n",
        "\n",
        "Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)).\n",
        "\n",
        "The `csv` dataset contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n",
        "\n",
        "We will perform a number of operations to gain insights from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1.0) Suggested/Required Imports"
      ],
      "metadata": {
        "id": "jlssd-CQOOXE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pm74v1u4d6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5d99bc-33e5-4014-81f1-f39195e612b4"
      },
      "source": [
        "# suggested imports\n",
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "tqdm.pandas()\n",
        "from ast import literal_eval\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfNsDQ253nzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d087fe-3dc7-4655-956f-b032c9b81ffe"
      },
      "source": [
        "from urllib import request\n",
        "import pandas as pd\n",
        "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "df = pd.read_csv('data_portfolio_22.csv')\n",
        "# this fills empty cells with empty strings\n",
        "df = df.fillna('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "CNfbxg2X3nzK",
        "outputId": "15f32077-7dd3-43be-be7d-4a5b887f0bdc"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       author            posted_at  num_comments  score selftext  \\\n",
              "0  -Howitzer-  2020-08-17 20:26:04            19      1            \n",
              "1  -Howitzer-  2020-07-06 17:01:48             1      3            \n",
              "2  -Howitzer-  2020-09-09 02:29:02             3      1            \n",
              "3  -Howitzer-  2020-06-23 23:02:39             2      1            \n",
              "4  -Howitzer-  2020-08-07 04:13:53            32    622            \n",
              "\n",
              "  subr_created_at              subr_description  \\\n",
              "0      2009-04-29  Subreddit about Donald Trump   \n",
              "1      2009-04-29  Subreddit about Donald Trump   \n",
              "2      2009-04-29  Subreddit about Donald Trump   \n",
              "3      2009-04-29  Subreddit about Donald Trump   \n",
              "4      2009-04-29  Subreddit about Donald Trump   \n",
              "\n",
              "                                       subr_faved_by  subr_numb_members  \\\n",
              "0  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "1  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "2  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "3  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "4  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "\n",
              "   subr_numb_posts    subreddit  \\\n",
              "0           796986  donaldtrump   \n",
              "1           796986  donaldtrump   \n",
              "2           796986  donaldtrump   \n",
              "3           796986  donaldtrump   \n",
              "4           796986  donaldtrump   \n",
              "\n",
              "                                               title  total_awards_received  \\\n",
              "0  BREAKING: Trump to begin hiding in mailboxes t...                      0   \n",
              "1                                Joe Biden's America                      0   \n",
              "2  4 more years and we can erase his legacy for g...                      0   \n",
              "3  Revelation 9:6 [Transhumanism: The New Religio...                      0   \n",
              "4                                     LOOK HERE, FAT                      0   \n",
              "\n",
              "   upvote_ratio  user_num_posts user_registered_at  user_upvote_ratio  \n",
              "0          1.00            4661         2012-11-09          -0.658599  \n",
              "1          0.67            4661         2012-11-09          -0.658599  \n",
              "2          1.00            4661         2012-11-09          -0.658599  \n",
              "3          1.00            4661         2012-11-09          -0.658599  \n",
              "4          0.88            4661         2012-11-09          -0.658599  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a68169cb-e2a6-4606-a513-1769b9c7bf54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>posted_at</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>selftext</th>\n",
              "      <th>subr_created_at</th>\n",
              "      <th>subr_description</th>\n",
              "      <th>subr_faved_by</th>\n",
              "      <th>subr_numb_members</th>\n",
              "      <th>subr_numb_posts</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>total_awards_received</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>user_num_posts</th>\n",
              "      <th>user_registered_at</th>\n",
              "      <th>user_upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-17 20:26:04</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-07-06 17:01:48</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Joe Biden's America</td>\n",
              "      <td>0</td>\n",
              "      <td>0.67</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-09-09 02:29:02</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>4 more years and we can erase his legacy for g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-06-23 23:02:39</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-07 04:13:53</td>\n",
              "      <td>32</td>\n",
              "      <td>622</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>LOOK HERE, FAT</td>\n",
              "      <td>0</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a68169cb-e2a6-4606-a513-1769b9c7bf54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a68169cb-e2a6-4606-a513-1769b9c7bf54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a68169cb-e2a6-4606-a513-1769b9c7bf54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQyR27z48nr"
      },
      "source": [
        "## P1.1 - Text data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfpA4Z5ADdok"
      },
      "source": [
        "### P1.1.1 - Offensive authors per subreddit \n",
        "\n",
        "As you will see, the dataset contains a lot of strings of the form `[***]`. These have been used to mask (or remove) swearwords to make it less offensive. We are interested in finding those users that have posted at least one swearword in each subreddit. We do this by counting occurrences of the `[***]` string in the `selftext` column (we can assume that an occurrence of `[***]` equals a swearword in the original dataset).\n",
        "\n",
        "**What to implement:** A function `offensive_authors(df)` that takes as input the original dataframe and returns a dataframe of the form below, where each row contains authors that posted at least one swearword in the corresponding subreddit.\n",
        "\n",
        "```\n",
        "subreddit\tauthor\n",
        "0\t40kLore\tCross_Ange\n",
        "1\t40kLore\tDaRandomGitty2\n",
        "2\t40kLore\tEMB1981\n",
        "3\t40kLore\tEvoxrus_XV\n",
        "4\t40kLore\tGrtrshop\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def offensive_authors(df):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns a DataFrame of authors who have used at least one swearword in their posts on each subreddit.\n",
        "\n",
        "    Args:\n",
        "    - df (pandas.DataFrame): A DataFrame containing at least the columns 'subreddit', 'author', and 'selftext'.\n",
        "    \n",
        "    Returns:\n",
        "    - pandas.DataFrame: A DataFrame containing the 'subreddit' and 'author' columns of authors who have used at least one swearword in their posts on each subreddit.\n",
        "    \"\"\"\n",
        "\n",
        "  # Group the dataframe by subreddit and author\n",
        "    groups = df.groupby(['subreddit', 'author'])\n",
        "\n",
        "    # Count the number of occurrences of [*] in the selftext column\n",
        "    counts = groups['selftext'].apply(lambda x: x.str.contains(r'\\[\\*\\*\\*\\]').sum())\n",
        "\n",
        "    # Filter the groups to only include those with at least one swearword\n",
        "    filtered_groups = groups.filter(lambda x: counts.loc[(x['subreddit'], x['author'])] > 0)\n",
        "\n",
        "    # Group the filtered groups by subreddit and author again\n",
        "    filtered_groups = filtered_groups.groupby(['subreddit', 'author'])\n",
        "\n",
        "    # Create the output dataframe\n",
        "    output_df = filtered_groups.size().reset_index(name='count')\n",
        "    output_df=output_df[['subreddit','author']]\n",
        "\n",
        "    return output_df"
      ],
      "metadata": {
        "id": "9oby6Xbz59F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offensive_authors(df)"
      ],
      "metadata": {
        "id": "FdiKSgWa9dKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "7f4c74c9-afac-4783-8ccc-9c00c63d4bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         subreddit              author\n",
              "0          40kLore          Cross_Ange\n",
              "1          40kLore      DaRandomGitty2\n",
              "2          40kLore             EMB1981\n",
              "3          40kLore          Evoxrus_XV\n",
              "4          40kLore            Grtrshop\n",
              "..             ...                 ...\n",
              "490  worldbuilding         nyello-2000\n",
              "491  worldbuilding        spirtomb1831\n",
              "492  worldbuilding      storywriter109\n",
              "493          xqcow  Stubka_The_Russian\n",
              "494          xqcow            marvi444\n",
              "\n",
              "[495 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b444519-6918-4a82-802a-3c4ef00e21c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>Cross_Ange</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>DaRandomGitty2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>EMB1981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>Evoxrus_XV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>Grtrshop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>worldbuilding</td>\n",
              "      <td>nyello-2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>worldbuilding</td>\n",
              "      <td>spirtomb1831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>worldbuilding</td>\n",
              "      <td>storywriter109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>xqcow</td>\n",
              "      <td>Stubka_The_Russian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>xqcow</td>\n",
              "      <td>marvi444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>495 rows √ó 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b444519-6918-4a82-802a-3c4ef00e21c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b444519-6918-4a82-802a-3c4ef00e21c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b444519-6918-4a82-802a-3c4ef00e21c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZ3u5aS3rrm"
      },
      "source": [
        "### P1.1.2 - Most common trigrams per subreddit\n",
        "\n",
        "We are interested in learning about _the ten most frequent trigrams_ (a [trigram](https://en.wikipedia.org/wiki/Trigram) is a sequence of three consecutive words) in each subreddit's content. You must compute these trigrams on both the `selftext` and `title` columns. Your task is to generate a Python dictionary of the form:\n",
        "\n",
        "```\n",
        "{subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "...\n",
        "subreddit63: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],}\n",
        "```\n",
        "\n",
        "That is, for each subreddit, the 10 most frequent trigrams and their frequency, stored in a list of tuples. Each trigram will be stored also as a tuple containing 3 strings.\n",
        "\n",
        "**What to implement**: A function `get_tris(df, stopwords_list, punctuation_list)` that will take as input the original dataframe, a list of stopwords and a list of punctuation signs (e.g., `?` or `!`), and will return a python dictionary with the above format. Your function must implement the following steps in order:\n",
        "\n",
        "-Create a new dataframe called `newdf` with only `subreddit`, `title` and `selftext` columns.\n",
        "-Add a new column to `newdf` called `full_text`, which will contain `title` and `selftext` concatenated with the string `.` (a full stop) followed by a space. That, is `A simple title` and `This is a text body` would be `A simple title. This is a text body`.\n",
        "-Remove all occurrences of the following strings from `full_text`. You must do this without creating a new column:\n",
        "  - `[***]`\n",
        "  - `&amp;`\n",
        "  - `&gt;`\n",
        "  - `https`\n",
        "- You must also remove all occurrences of at least three consecutive hyphens, for example, you should remove strings like `---`, `----`, `-----`, etc., but not `--` and not `-`.\n",
        "- Tokenize the contents of the `full_text` column after lower casing (removing all capitalization). You should use the `word_tokenize` function in `nltk`. Add the results to a new column called `full_text_tokenized`.\n",
        "-Remove all tokens that are either stopwords or punctuation from `full_text_tokenized` and store the results in a new column called `full_text_tokenized_clean`. _See Note 1_.\n",
        "-Create a new dataframe called `adf` (which will stand for _aggregated dataframe_), which will have one row per subreddit (i.e., 63 rows), and will have two columns: `subreddit` (the subreddit name), and `all_words`, which will be a big list with all the words that belong to that subreddit as extracted from the `full_text_tokenized_clean`.\n",
        "-Obtain trigram counts, which will be stored in a dictionary where each `key` will be a trigram (a `tuple` containing 3 consecutive tokens), and each `value` will be their overall frequency in that subreddit. You are  encouraged to use functions from the `nltk` package, although you can choose any approach to solve this part.\n",
        "-Finally, use the information you have in `adf` for generating the desired dictionary, and return it. _See Note 2_.\n",
        "\n",
        "Note 1. You can obtain stopwords and punctuation as follows.\n",
        "- Stopwords: \n",
        "```\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "```\n",
        "- Punctuation:\n",
        "```\n",
        "import string\n",
        "punctuation = list(string.punctuation)\n",
        "```\n",
        "\n",
        "Note 2. You do not have to apply an additional ordering when there are several trigrams with the same frequency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports here for extra clarity\n",
        "from nltk.corpus import stopwords as sw\n",
        "import string\n",
        "import warnings\n",
        "from nltk import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "def get_tris(df, stopwords_list, punctuation_list):\n",
        "\n",
        "  \"\"\"Returns a dictionary of the top 10 trigrams for each subreddit in the input DataFrame, after preprocessing the text data.\n",
        "\n",
        "    Args:\n",
        "    - df (pandas.DataFrame): A DataFrame containing at least the columns 'subreddit', 'title', and 'selftext'.\n",
        "    - stopwords_list (list): A list of stopwords to be removed from the text data.\n",
        "    - punctuation_list (list): A list of punctuation symbols to be removed from the text data.\n",
        "    \n",
        "    Returns:\n",
        "    - dict: A dictionary where the keys are the subreddits in the input DataFrame, and the values are lists of the top 10 trigrams in the preprocessed text data for each subreddit.\n",
        "  \"\"\"\n",
        "    \n",
        "  # create new df with only relevant columns\n",
        "  newdf=df[['subreddit','title','selftext']]\n",
        "  \n",
        "  # concatenate title and selftext\n",
        "  newdf['full_text'] = newdf['title'] + '. ' + newdf['selftext']\n",
        "  \n",
        "  removal_strings=re.compile(r'\\[\\*\\*\\*\\]|&amp;|&gt;|https')  #to search either and all of the strings in the text\n",
        "  newdf['full_text'] = newdf['full_text'].replace(removal_strings, '', regex=True)\n",
        "\n",
        "  # for regex replacement - remove the strings \"[***]\", \"&amp;\", \"&gt;\" and \"https\", also at least three consecutive dashes\n",
        "  hyphen_pattern=re.compile(r\"(-{3,})\")  #to search three or more hyphens\n",
        "  newdf[\"full_text\"] = newdf[\"full_text\"].str.replace(hyphen_pattern, \"\")\n",
        "  \n",
        "  # lower case, tokenize, and add result to full_text_tokenize\n",
        "  newdf['full_text_tokenized'] = newdf['full_text'].apply(lambda x:word_tokenize(x.lower()))\n",
        "  \n",
        "  # clean the full_text_tokenized column by iterating over each word and discarding if it's either a stopword or punctuation\n",
        "  def clean_tokens(tokens):\n",
        "    \"\"\"\n",
        "    This function takes a list of tokens and removes any tokens that appear in a given list of stop words or a given list of punctuation marks.\n",
        "    \n",
        "    Parameters:\n",
        "    - tokens (list): A list of tokens that need to be cleaned up.\n",
        "    - stopwords_list (list): A list of stop words to be removed from the tokens.\n",
        "    - punctuation_list (list): A list of punctuation marks to be removed from the tokens.\n",
        "    \n",
        "    Returns:\n",
        "    - list: A list of cleaned tokens, where any tokens that appear in the provided stop words or punctuation lists are removed.\n",
        "    \"\"\"\n",
        "    return [token for token in tokens if token not in stopwords_list and token not in punctuation_list]\n",
        "    \n",
        "  newdf['cleaned_text_tokenized'] = newdf['full_text_tokenized'].apply(clean_tokens)\n",
        "  \n",
        "  # create new aggregated dataframe by concatenating all full_text_tokenized_clean values - rename columns as requested\n",
        "  # group by subreddit and concatenate all the cleaned token lists\n",
        "  adf = newdf.groupby('subreddit')['cleaned_text_tokenized'].sum()\n",
        "\n",
        "  # convert to a dataframe and reset the index to get the subreddit as a column\n",
        "  adf = adf.to_frame().reset_index()\n",
        "\n",
        "  # rename the column with the aggregated tokens to 'all_words'\n",
        "  adf = adf.rename(columns={'cleaned_text_tokenized': 'all_words'})\n",
        "  \n",
        "  # create new Series object by piping nltk's FreqDist and trigrams functions into all_words\n",
        "  trigrams=[]\n",
        "  for word in adf['all_words']:\n",
        "    trigrams.extend(ngrams(word,3))\n",
        "\n",
        "  trigram_counts=Counter(trigrams) #New series object is trigram_counts\n",
        "  \n",
        "  # create output dictionary by zipping subreddit column from adf and tri_counts into a list of tuples, then passing dict()\n",
        "  tri_counts=adf['all_words'].apply(lambda x: FreqDist(ngrams(x,3)))\n",
        "  output_dict=dict(zip(adf['subreddit'], tri_counts.apply(lambda x:sorted(x.items(),key= lambda x: x[1], reverse=True)[:10])))\n",
        "  return output_dict\n",
        "  # the top 10 most frequent ngrams are obtained by calling sorted() on tri_counts and keeping only the top 10 elements\n",
        "  "
      ],
      "metadata": {
        "id": "A0NeN7uGftfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get stopwords as list\n",
        "sw = sw.words('english')\n",
        "# get punctuation as list\n",
        "p = list(string.punctuation)\n",
        "# optional lines for adding the below line to avoid the SettingWithCopyWarning\n",
        "warnings.filterwarnings('ignore')\n",
        "get_tris(df, sw, p)"
      ],
      "metadata": {
        "id": "jLHfnUK_g5vb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb9d79d-ef56-458a-a606-34ee7ca00539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'40kLore': [(('whose', 'bolter', 'anyway'), 8),\n",
              "  (('started', 'us', 'examples'), 8),\n",
              "  (('space', 'marine', 'chapter'), 7),\n",
              "  (('kabal', 'black', 'heart'), 7),\n",
              "  (('lo', '‚Äô', 'tos'), 7),\n",
              "  (('die', 'paragon', 'knights'), 6),\n",
              "  (('dark', 'age', 'technology'), 4),\n",
              "  (('let', \"'s\", 'say'), 4),\n",
              "  (('``', 'star', 'claimers'), 4),\n",
              "  (('star', 'claimers', \"''\"), 4)],\n",
              " 'AMD_Stock': [(('created', 'subreddit', 'reddit'), 10),\n",
              "  (('subreddit', 'reddit', 'posts'), 10),\n",
              "  (('reddit', 'posts', 'r/radeongpus'), 10),\n",
              "  (('open', 'redditors', 'posts'), 10),\n",
              "  (('redditors', 'posts', 'well'), 10),\n",
              "  (('posts', 'well', 'please'), 10),\n",
              "  (('well', 'please', 'consider'), 10),\n",
              "  (('please', 'consider', 'subscribing'), 10),\n",
              "  (('consider', 'subscribing', 'find'), 10),\n",
              "  (('subscribing', 'find', 'posts'), 10)],\n",
              " 'Anki': [(('``', 'conditional', \"''\"), 7),\n",
              "  (('``', 'field2', \"''\"), 5),\n",
              "  (('\\\\^conditional', 'field1', '/conditional'), 5),\n",
              "  (('conditional', \"''\", 'filled'), 4),\n",
              "  (('font-family', 'simplified', 'arabic'), 3),\n",
              "  (('simplified', 'arabic', 'font-size'), 3),\n",
              "  (('export', 'cards', 'selected'), 3),\n",
              "  (('cards', 'selected', 'browser'), 3),\n",
              "  (('anki', \"'s\", 'code'), 3),\n",
              "  (('conditional', 'field1', '/conditional'), 3)],\n",
              " 'ApexOutlands': [(('arc', 'star', 'arc'), 2),\n",
              "  (('star', 'arc', 'star'), 2),\n",
              "  (('cute', '‚Äú', 'mean'), 1),\n",
              "  (('‚Äú', 'mean', '..'), 1),\n",
              "  (('mean', '..', 'goo'), 1),\n",
              "  (('..', 'goo', 'goo'), 1),\n",
              "  (('goo', 'goo', 'ga'), 1),\n",
              "  (('goo', 'ga', 'ga'), 1),\n",
              "  (('ga', 'ga', '‚Äù'), 1),\n",
              "  (('ga', '‚Äù', 'like'), 1)],\n",
              " 'BanGDream': [(('bang', 'dream', 'girls'), 10),\n",
              "  (('boosts', 'score', 'notes'), 9),\n",
              "  (('full', 'english/romaji', 'lyrics'), 8),\n",
              "  (('low', 'effort', 'edit'), 7),\n",
              "  (('effort', 'edit', 'bang'), 7),\n",
              "  (('edit', 'bang', 'dream'), 7),\n",
              "  (('dream', 'girls', 'initial'), 7),\n",
              "  (('girls', 'initial', 'cars'), 7),\n",
              "  (('initial', 'cars', 'either'), 7),\n",
              "  (('cars', 'either', 'make'), 7)],\n",
              " 'BrandNewSentence': [(('cool', 'cool', 'cool'), 2),\n",
              "  (('unrealistic', 'de-dun-dun-dun', 'grindr'), 1),\n",
              "  (('de-dun-dun-dun', 'grindr', 'straight'), 1),\n",
              "  (('grindr', 'straight', 'guys'), 1),\n",
              "  (('straight', 'guys', \"n't\"), 1),\n",
              "  (('guys', \"n't\", 'start'), 1),\n",
              "  ((\"n't\", 'start', 'fire'), 1),\n",
              "  (('start', 'fire', 'hate'), 1),\n",
              "  (('fire', 'hate', 'apples'), 1),\n",
              "  (('hate', 'apples', 'avoid'), 1)],\n",
              " 'COVID': [(('covid-19', 'products', 'covid-19'), 6),\n",
              "  (('products', 'covid-19', 'test'), 6),\n",
              "  (('covid-19', 'test', 'kits'), 6),\n",
              "  (('test', 'kits', 'covid-19'), 6),\n",
              "  (('kits', 'covid-19', 'face'), 6),\n",
              "  (('covid-19', 'face', 'masks'), 6),\n",
              "  (('face', 'masks', 'protective'), 6),\n",
              "  (('masks', 'protective', 'isolation'), 6),\n",
              "  (('protective', 'isolation', 'coveralls'), 6),\n",
              "  (('isolation', 'coveralls', 'ppe'), 6)],\n",
              " 'COVID19': [(('coronavirus', 'disease', '2019'), 8),\n",
              "  (('get', 'in-depth', 'ysis'), 6),\n",
              "  (('in-depth', 'ysis', 'covid-19'), 6),\n",
              "  (('ysis', 'covid-19', 'impact'), 6),\n",
              "  (('disease', '2019', 'covid-19'), 4),\n",
              "  (('systematic', 'review', 'meta-ysis'), 4),\n",
              "  (('market', 'get', 'in-depth'), 4),\n",
              "  (('nebraska', 'medical', 'center'), 3),\n",
              "  (('among', 'covid-19', 'patients'), 2),\n",
              "  (('severe', 'covid-19', 'patients'), 2)],\n",
              " 'CanadaCoronavirus': [(('pending|', 'pending', '|self-isolating'), 64),\n",
              "  (('|pending|', 'pending', '|pending'), 57),\n",
              "  (('|pending', '|pending', '|pending'), 53),\n",
              "  (('lt', '5', 'may'), 50),\n",
              "  (('pending', '|pending', '|self-isolating'), 46),\n",
              "  (('lt', '5', 'lt'), 45),\n",
              "  (('5', 'lt', '5'), 45),\n",
              "  (('pending', '|pending', '|pending'), 42),\n",
              "  (('``', 'resolved', \"''\"), 32),\n",
              "  (('toronto|', 'pending|', 'pending'), 30)],\n",
              " 'China_Flu': [(('covid-19', 'daily', 'discussion'), 36),\n",
              "  (('world', 'health', 'organization'), 14),\n",
              "  (('covid-19', 'weekly', 'discussion'), 12),\n",
              "  (('growth', 'üìà', 'increasing'), 10),\n",
              "  (('total', 'confirmed', 'cases'), 10),\n",
              "  (('new', 'cases', 'coronavirus'), 8),\n",
              "  (('confirmed', 'cases', 'growth'), 8),\n",
              "  (('death', 'rate', 'growth'), 8),\n",
              "  (('discord', 'server', '//www.discord.gg/yjw2rky'), 7),\n",
              "  (('available', 'via', 'internet'), 7)],\n",
              " 'Coronavirus': [(('coronavirus', 'death', 'toll'), 29),\n",
              "  (('new', 'coronavirus', 'cases'), 27),\n",
              "  (('new', 'covid-19', 'cases'), 21),\n",
              "  (('new', 'york', 'city'), 20),\n",
              "  (('tests', 'positive', 'coronavirus'), 16),\n",
              "  (('tested', 'positive', 'covid-19'), 15),\n",
              "  (('new', 'cases', 'coronavirus'), 13),\n",
              "  (('test', 'positive', 'covid-19'), 10),\n",
              "  (('tested', 'positive', 'coronavirus'), 10),\n",
              "  (('tests', 'positive', 'covid-19'), 9)],\n",
              " 'CoronavirusCA': [(('daily', 'check-in/personal', 'thread'), 8),\n",
              "  (('2020', 'concerns', 'vents'), 4),\n",
              "  (('concerns', 'vents', 'questions'), 4),\n",
              "  (('vents', 'questions', 'anecdotes'), 4),\n",
              "  (('questions', 'anecdotes', 'personal'), 4),\n",
              "  (('anecdotes', 'personal', 'preparation'), 4),\n",
              "  (('personal', 'preparation', 'understand'), 4),\n",
              "  (('preparation', 'understand', 'stressful'), 4),\n",
              "  (('understand', 'stressful', 'time'), 4),\n",
              "  (('stressful', 'time', 'community'), 4)],\n",
              " 'CoronavirusCirclejerk': [(('average', 'daily', 'number'), 2),\n",
              "  (('daily', 'number', 'deaths'), 2),\n",
              "  (('number', 'deaths', 'per'), 2),\n",
              "  (('deaths', 'per', 'week'), 2),\n",
              "  (('suggestions', 'better', 'unenforceable'), 1),\n",
              "  (('better', 'unenforceable', 'orders'), 1),\n",
              "  (('unenforceable', 'orders', 'removed'), 1),\n",
              "  (('orders', 'removed', '‚Äô'), 1),\n",
              "  (('removed', '‚Äô', 'like'), 1),\n",
              "  (('‚Äô', 'like', 'casey'), 1)],\n",
              " 'CoronavirusDownunder': [(('27', '26', '25'), 26),\n",
              "  (('-|', '-|', '-|'), 23),\n",
              "  (('sept', '27', '26'), 23),\n",
              "  (('26', '25', '24'), 23),\n",
              "  (('lt', '==', '10'), 22),\n",
              "  (('==', '10', 'days'), 22),\n",
              "  (('10', 'days', 'ago'), 22),\n",
              "  (('24', '23', '22'), 19),\n",
              "  (('25', '24', '23'), 18),\n",
              "  (('aged', 'care', 'facility'), 13)],\n",
              " 'CoronavirusUK': [(('latest', 'r', 'number'), 18),\n",
              "  (('daily', 'deaths', 'total'), 11),\n",
              "  (('nhs', 'test', 'trace'), 10),\n",
              "  (('nhs',\n",
              "    'england',\n",
              "    '//www.england.nhs.uk/statistics/statistical-work-areas/covid-19-daily-deaths/'),\n",
              "   9),\n",
              "  (('daily', 'deaths', '0'), 9),\n",
              "  (('deaths', '0', 'total'), 9),\n",
              "  (('test', 'trace', 'ask'), 9),\n",
              "  (('trace', 'ask', 'test'), 9),\n",
              "  (('ask', 'test', 'online'), 9),\n",
              "  (('//www.gov.scot/publications/coronavirus-covid-19-daily-data-for-scotland/',\n",
              "    'daily',\n",
              "    'deaths'),\n",
              "   8)],\n",
              " 'CoronavirusUS': [(('growth', 'üìà', 'increasing'), 9),\n",
              "  (('confirmed', 'cases', 'growth'), 8),\n",
              "  (('total', 'confirmed', 'cases'), 8),\n",
              "  (('death', 'rate', 'growth'), 8),\n",
              "  (('cases', 'growth', 'üìà'), 6),\n",
              "  (('rate', 'growth', 'mixed'), 5),\n",
              "  (('recovery', 'rate', 'üìâ'), 5),\n",
              "  (('update', 'coronavirus', 'growth'), 4),\n",
              "  (('data', 'available', 'detail'), 4),\n",
              "  (('available',\n",
              "    'detail',\n",
              "    '//docs.google.com/spreadsheets/d/1w7kqwpsncplcckokvbng8a2nwcn5qo3ydycdcvw38cc/edit'),\n",
              "   4)],\n",
              " 'CovIdiots': [(('refusing', 'wear', 'mask'), 3),\n",
              "  (('people', \"'s\", 'faces'), 2),\n",
              "  (('tests', 'positive', 'covid-19'), 2),\n",
              "  (('staff', 'nursing', 'home'), 1),\n",
              "  (('nursing', 'home', '19'), 1),\n",
              "  (('home', '19', 'died'), 1),\n",
              "  (('19', 'died', 'told'), 1),\n",
              "  (('died', 'told', 'masks'), 1),\n",
              "  (('told', 'masks', 'would'), 1),\n",
              "  (('masks', 'would', 'scare'), 1)],\n",
              " 'CrackheadCraigslist': [(('deal', 'beauty', 'obsession'), 2),\n",
              "  (('much', 'dude', 'looks'), 1),\n",
              "  (('dude', 'looks', 'like'), 1),\n",
              "  (('looks', 'like', 'jane'), 1),\n",
              "  (('like', 'jane', 'knows'), 1),\n",
              "  (('jane', 'knows', 'wants'), 1),\n",
              "  (('knows', 'wants', '....'), 1),\n",
              "  (('wants', '....', 'looks'), 1),\n",
              "  (('....', 'looks', 'authentic'), 1),\n",
              "  (('looks', 'authentic', 'hmmmm'), 1)],\n",
              " 'EngineeringStudents': [(('resume', 'roundtable', \"'re\"), 1),\n",
              "  (('roundtable', \"'re\", 'prepare'), 1),\n",
              "  ((\"'re\", 'prepare', 'career'), 1),\n",
              "  (('prepare', 'career', 'fairs'), 1),\n",
              "  (('career', 'fairs', 'career'), 1),\n",
              "  (('fairs', 'career', 'fairs'), 1),\n",
              "  (('career', 'fairs', 'job'), 1),\n",
              "  (('fairs', 'job', 'applications'), 1),\n",
              "  (('job', 'applications', 'loom'), 1),\n",
              "  (('applications', 'loom', 'near'), 1)],\n",
              " 'FigureSkating': [(('russian', 'championships', '2021'), 3),\n",
              "  (('russian', 'figure', 'skating'), 3),\n",
              "  (('rostelecom', 'cup', '2020'), 2),\n",
              "  (('sp', 'russian', 'championships'), 2),\n",
              "  (('sp', 'russian', 'cup'), 2),\n",
              "  (('4th', 'stage', '2020'), 2),\n",
              "  (('best', 'men', \"'s\"), 2),\n",
              "  (('1.', 'yuzuru', 'hanyu'), 2),\n",
              "  (('yuzuru', 'hanyu', '2.'), 2),\n",
              "  (('us', 'nationals', '2021'), 2)],\n",
              " 'Fusion360': [(('fusion', '360', 'beginner'), 2),\n",
              "  (('360', 'beginner', 'tip'), 2),\n",
              "  (('beginner', 'tip', 'clear'), 2),\n",
              "  (('tip', 'clear', 'selections'), 2),\n",
              "  (('clear', 'selections', 'start'), 2),\n",
              "  (('selections', 'start', 'fusion'), 2),\n",
              "  (('start', 'fusion', '360'), 2),\n",
              "  (('fusion', '360', 'increments'), 1),\n",
              "  (('360', 'increments', 'adjust'), 1),\n",
              "  (('increments', 'adjust', 'drag'), 1)],\n",
              " 'Gameboy': [(('despite', 'several', 'modded'), 1),\n",
              "  (('several', 'modded', 'dmgs'), 1),\n",
              "  (('modded', 'dmgs', 'sometimes'), 1),\n",
              "  (('dmgs', 'sometimes', 'like'), 1),\n",
              "  (('sometimes', 'like', 'going'), 1),\n",
              "  (('like', 'going', 'back'), 1),\n",
              "  (('going', 'back', 'humble'), 1),\n",
              "  (('back', 'humble', 'stock'), 1),\n",
              "  (('humble', 'stock', 'game'), 1),\n",
              "  (('stock', 'game', 'boy'), 1)],\n",
              " 'HolUp': [(('p', 'color', 'blind'), 10),\n",
              "  (('color', 'blind', 'test'), 10),\n",
              "  (('blind', 'test', 'p'), 9),\n",
              "  (('test', 'p', 'color'), 9),\n",
              "  (('holup', 'holup', 'holup'), 7),\n",
              "  (('hol', 'hol', 'hol'), 6),\n",
              "  (('wait', 'minute', '....'), 3),\n",
              "  (('minute', '....', 'wait'), 3),\n",
              "  (('wait', 'minute', '...'), 3),\n",
              "  ((\"'s\", 'wrong', \"'s\"), 2)],\n",
              " 'JoeBiden': [(('joe', 'biden', \"'s\"), 7),\n",
              "  (('..', 'joe', 'biden'), 5),\n",
              "  (('joe', 'biden', 'president'), 5),\n",
              "  (('responded', '``', 'true'), 4),\n",
              "  (('``', 'true', \"''\"), 4),\n",
              "  (('covid', 'covid', 'covid'), 4),\n",
              "  (('state', 'covid', 'deaths'), 4),\n",
              "  (('joe', 'biden', 'kamala'), 3),\n",
              "  (('biden', 'kamala', 'harris'), 3),\n",
              "  (('endorse', 'joe', 'biden'), 3)],\n",
              " 'Konosuba': [(('random', 'thought', 'aqua'), 2),\n",
              "  (('megumin', 'drawn', 'Áõ∏Â∑ù„Çä„Çá„ÅÜ'), 2),\n",
              "  (('took', 'step', 'back'), 2),\n",
              "  (('read', 'comments', 'mom'), 2),\n",
              "  (('megumin', 'vanir', 'wiz'), 1),\n",
              "  (('vanir', 'wiz', 'megumin'), 1),\n",
              "  (('wiz', 'megumin', 'kitsu'), 1),\n",
              "  (('megumin', 'kitsu', 'darkness'), 1),\n",
              "  (('kitsu', 'darkness', 'megumin'), 1),\n",
              "  (('darkness', 'megumin', 'smol'), 1)],\n",
              " 'LivestreamFail': [(('walkthrough', 'gameplay', 'part'), 9),\n",
              "  (('erobb', \"'s\", 'voice'), 3),\n",
              "  ((\"'s\", 'voice', 'cracks'), 3),\n",
              "  (('potty', 'train', 'puppy'), 3),\n",
              "  (('train', 'puppy', 'easily'), 3),\n",
              "  (('puppy', 'easily', 'everything'), 3),\n",
              "  (('easily', 'everything', 'need'), 3),\n",
              "  (('everything', 'need', 'know'), 3),\n",
              "  (('talks', 'first', 'time'), 3),\n",
              "  (('abusing', 'copyright', 'law'), 2)],\n",
              " 'LockdownSkepticism': [(('adjusted', 'odds', 'ratio'), 3),\n",
              "  (('herd', 'immunity', '‚Äô'), 2),\n",
              "  (('new', 'york', '‚Äô'), 2),\n",
              "  (('vaccine', 'wo', \"n't\"), 2),\n",
              "  (('oxford', 'professor', 'says'), 2),\n",
              "  (('great', 'barrington', 'declaration'), 2),\n",
              "  (('masks', 'social', 'distancing'), 2),\n",
              "  (('people', 'claiming', 'unemployment'), 2),\n",
              "  (('claiming', 'unemployment', 'benefit'), 2),\n",
              "  (('strip', 'club', 'lawsuit'), 2)],\n",
              " 'MensLib': [(('tuesday', 'check', \"'s\"), 1),\n",
              "  (('check', \"'s\", 'everyone'), 1),\n",
              "  ((\"'s\", 'everyone', \"'s\"), 1),\n",
              "  (('everyone', \"'s\", 'mental'), 1),\n",
              "  ((\"'s\", 'mental', 'health'), 1),\n",
              "  (('mental', 'health', 'good'), 1),\n",
              "  (('health', 'good', 'morning'), 1),\n",
              "  (('good', 'morning', 'everyone'), 1),\n",
              "  (('morning', 'everyone', 'welcome'), 1),\n",
              "  (('everyone', 'welcome', 'new'), 1)],\n",
              " 'NintendoSwitch': [(('daily', 'question', 'thread'), 10),\n",
              "  (('animal', 'crossing', 'new'), 9),\n",
              "  (('crossing', 'new', 'horizons'), 8),\n",
              "  (('/r/nintendoswitch', \"'s\", 'daily'), 8),\n",
              "  ((\"'s\", 'daily', 'question'), 8),\n",
              "  ((\"'re\", 'interested', 'becoming'), 8),\n",
              "  (('interested', 'becoming', 'wiki'), 8),\n",
              "  (('becoming', 'wiki', 'contributor'), 8),\n",
              "  (('wiki', 'contributor', 'message'), 8),\n",
              "  (('contributor', 'message', '/u/flapsnapple'), 8)],\n",
              " 'NoLockdownsNoMasks': [(('thailand', 'medical', 'news'), 2),\n",
              "  (('government', 'model', 'suggests'), 1),\n",
              "  (('model', 'suggests', 'u.s.'), 1),\n",
              "  (('suggests', 'u.s.', 'covid-19'), 1),\n",
              "  (('u.s.', 'covid-19', 'cases'), 1),\n",
              "  (('covid-19', 'cases', 'could'), 1),\n",
              "  (('cases', 'could', 'approaching'), 1),\n",
              "  (('could', 'approaching', '100'), 1),\n",
              "  (('approaching', '100', 'million'), 1),\n",
              "  (('100', 'million', 'npr'), 1)],\n",
              " 'NoNewNormal': [(('``', 'covid', \"''\"), 6),\n",
              "  (('``', 'new', 'normal'), 5),\n",
              "  (('new', 'normal', \"''\"), 5),\n",
              "  (('‚Äù', '‚Äú', '‚Äô'), 4),\n",
              "  (('word', '``', 'covid'), 4),\n",
              "  (('``', 'great', 'reset'), 4),\n",
              "  (('great', 'reset', \"''\"), 4),\n",
              "  (('people', 'wearing', 'masks'), 3),\n",
              "  (('....', \"''\", \"'m\"), 3),\n",
              "  ((\"''\", \"'m\", 'sorry'), 3)],\n",
              " 'Norse': [(('viking', 'amulet', 'dragon'), 1),\n",
              "  (('amulet', 'dragon', 'fafnir'), 1),\n",
              "  (('dragon', 'fafnir', 'recreated'), 1),\n",
              "  (('fafnir', 'recreated', 'amulets'), 1),\n",
              "  (('recreated', 'amulets', 'usually'), 1),\n",
              "  (('amulets', 'usually', 'referred'), 1),\n",
              "  (('usually', 'referred', 'viking'), 1),\n",
              "  (('referred', 'viking', 'age'), 1),\n",
              "  (('viking', 'age', 'votive'), 1),\n",
              "  (('age', 'votive', 'thor'), 1)],\n",
              " 'PaymoneyWubby': [(('5', '8', \"''\"), 2),\n",
              "  (('funny', 'clips', 'tonight'), 2),\n",
              "  (('discord', 'trying', 'find'), 1),\n",
              "  (('trying', 'find', 'songs'), 1),\n",
              "  (('find', 'songs', 'played'), 1),\n",
              "  (('songs', 'played', 'stream'), 1),\n",
              "  (('played', 'stream', 'today'), 1),\n",
              "  (('stream', 'today', \"n't\"), 1),\n",
              "  (('today', \"n't\", 'popping'), 1),\n",
              "  ((\"n't\", 'popping', 'media'), 1)],\n",
              " 'Pizza': [(('new', 'york', 'style'), 3),\n",
              "  (('months', 'ago', 'sister'), 1),\n",
              "  (('ago', 'sister', 'tried'), 1),\n",
              "  (('sister', 'tried', 'hand'), 1),\n",
              "  (('tried', 'hand', 'deep-dish'), 1),\n",
              "  (('hand', 'deep-dish', 'birthday'), 1),\n",
              "  (('deep-dish', 'birthday', 'known'), 1),\n",
              "  (('birthday', 'known', 'sub'), 1),\n",
              "  (('known', 'sub', 'would'), 1),\n",
              "  (('sub', 'would', 'taken'), 1)],\n",
              " 'PublicFreakout': [(('black', 'lives', 'matter'), 7),\n",
              "  (('ÿØÿßŸÜŸÑŸàÿØ', 'ÿ¢ŸáŸÜ⁄Ø', 'ŸÖÿßÿ≤ŸÜÿØÿ±ÿßŸÜ€å'), 4),\n",
              "  (('new', 'york', 'city'), 3),\n",
              "  (('body', 'cam', 'footage'), 3),\n",
              "  (('p', 'color', 'blind'), 3),\n",
              "  (('color', 'blind', 'test'), 3),\n",
              "  (('ÿØÿßŸÜŸÑŸàÿØ', 'ÿ¢ŸáŸÜ⁄Ø', '⁄©ÿ±ÿØ€å'), 3),\n",
              "  (('lives', 'matter', 'protest'), 2),\n",
              "  (('gets', 'racially', 'profiled'), 2),\n",
              "  (('george', 'floyd', 'protests'), 2)],\n",
              " 'SandersForPresident': [(('early', 'voting', 'ends'), 6),\n",
              "  (('bernie', 'sanders', 'president'), 4),\n",
              "  (('bernie', 'sanders', '‚Äô'), 4),\n",
              "  (('let', \"'s\", 'get'), 3),\n",
              "  (('bernie', 'sanders', 'nevada'), 3),\n",
              "  (('sen.', 'bernie', 'sanders'), 3),\n",
              "  (('democrats', 'abroad', 'primary'), 3),\n",
              "  (('bernie', 'sanders', 'takes'), 3),\n",
              "  (('bernie', 'sanders', 'bernie'), 3),\n",
              "  ((\"''\", 'bernie', 'sanders'), 3)],\n",
              " 'TheRealJoke': [(('hitting', 'high', 'note'), 1),\n",
              "  (('high', 'note', 'got'), 1),\n",
              "  (('note', 'got', 'ta'), 1),\n",
              "  (('got', 'ta', 'mother'), 1),\n",
              "  (('ta', 'mother', 'real'), 1),\n",
              "  (('mother', 'real', 'jokes'), 1),\n",
              "  (('real', 'jokes', 'guy'), 1),\n",
              "  (('jokes', 'guy', 'took'), 1),\n",
              "  (('guy', 'took', 'comment'), 1),\n",
              "  (('took', 'comment', 'whole'), 1)],\n",
              " 'TheVampireDiaries': [(('surfer', 'wolf', 'guy'), 4),\n",
              "  (('``', 'agree', \"''\"), 3),\n",
              "  (('‚Äô', 'sure', 'one'), 2),\n",
              "  (('‚Äô', 'ever', 'seen'), 2),\n",
              "  (('really', 'happened', 'night'), 2),\n",
              "  (('damon', 'killed', 'surfer'), 2),\n",
              "  (('killed', 'surfer', 'wolf'), 2),\n",
              "  (('``', 'accept', 'opinion'), 2),\n",
              "  (('accept', 'opinion', \"''\"), 2),\n",
              "  (('yes', 'klaus', 'caroline'), 1)],\n",
              " 'WTF': [(('sure', 'sneak', 'one'), 1),\n",
              "  (('sneak', 'one', '..'), 1),\n",
              "  (('one', '..', \"'m\"), 1),\n",
              "  (('..', \"'m\", 'march'), 1),\n",
              "  ((\"'m\", 'march', \"'m\"), 1),\n",
              "  (('march', \"'m\", 'going'), 1),\n",
              "  ((\"'m\", 'going', 'need'), 1),\n",
              "  (('going', 'need', 'llama'), 1),\n",
              "  (('need', 'llama', 'firefighters'), 1),\n",
              "  (('llama', 'firefighters', 'surprised'), 1)],\n",
              " 'WindowsMR': [(('hard', 'find', 'replacement'), 1),\n",
              "  (('find', 'replacement', 'controllers'), 1),\n",
              "  (('replacement', 'controllers', 'wmr'), 1),\n",
              "  (('controllers', 'wmr', 'good'), 1),\n",
              "  (('wmr', 'good', 'concept'), 1),\n",
              "  (('good', 'concept', 'replacement'), 1),\n",
              "  (('concept', 'replacement', 'parts'), 1),\n",
              "  (('replacement', 'parts', 'impossible'), 1),\n",
              "  (('parts', 'impossible', 'find'), 1)],\n",
              " 'WitchesVsPatriarchy': [(('``', 'women', 'history'), 3),\n",
              "  (('women', 'history', \"''\"), 3),\n",
              "  (('today', '``', 'women'), 2),\n",
              "  (('given', 'success', '``'), 1),\n",
              "  (('success', '``', 'women'), 1),\n",
              "  (('history', \"''\", 'fictional'), 1),\n",
              "  ((\"''\", 'fictional', 'character'), 1),\n",
              "  (('fictional', 'character', 'entry'), 1),\n",
              "  (('character', 'entry', 'april'), 1),\n",
              "  (('entry', 'april', '1st'), 1)],\n",
              " '[***]og': [(('kodak', 'gold', '200'), 51),\n",
              "  (('kodak', 'portra', '400'), 41),\n",
              "  (('kodak', 'ultramax', '400'), 19),\n",
              "  (('canon', 'ae-1', '50mm'), 15),\n",
              "  (('kodak', 'ektar', '100'), 14),\n",
              "  (('portra', '400', 'mamiya'), 13),\n",
              "  (('fuji', 'superia', '400'), 13),\n",
              "  (('mamiya', 'rb67', 'portra'), 13),\n",
              "  (('rb67', 'portra', '400'), 12),\n",
              "  (('canon', 'eos', '3'), 12)],\n",
              " 'army': [(('fort', 'carson', \"'s\"), 5),\n",
              "  (('fort', 'carson', 'outdoor'), 4),\n",
              "  (('carson', 'outdoor', 'recreation'), 4),\n",
              "  (('mountain', 'post', 'living'), 4),\n",
              "  (('fort', 'carson', 'mountain'), 3),\n",
              "  (('carson', 'mountain', 'post'), 3),\n",
              "  (('post', 'living', 'pcs'), 3),\n",
              "  (('wednesday', 'advice', 'thread'), 2),\n",
              "  (('let', 'us', 'know'), 2),\n",
              "  ((\"'best\", \"'worst\", 'mos'), 2)],\n",
              " 'bleach': [(('court', 'guard', 'squads'), 12),\n",
              "  (('1st', 'level', 'hell'), 7),\n",
              "  (('13', 'court', 'guard'), 7),\n",
              "  (('court', 'guard', 'squad'), 7),\n",
              "  (('traditional', 'soul', 'reaper'), 7),\n",
              "  (('2nd', 'level', 'hell'), 6),\n",
              "  (('3rd', 'level', 'hell'), 6),\n",
              "  (('guard', 'squads', 'hell'), 6),\n",
              "  (('lowest', 'level', 'hell'), 6),\n",
              "  (('guard', 'squad', 'hell'), 6)],\n",
              " 'brisbane': [(('..', 'cloud', 'report'), 5),\n",
              "  (('prinl', 'place', 'residence'), 4),\n",
              "  (('business', 'activity', 'undertaking'), 4),\n",
              "  (('drink', 'ice', 'cold'), 3),\n",
              "  (('ice', 'cold', 'vb'), 3),\n",
              "  (('cold', 'vb', 'bar'), 3),\n",
              "  (('vb', 'bar', 'fridge'), 3),\n",
              "  (('year', \"'s\", 'ekka'), 2),\n",
              "  ((\"'s\", 'ekka', 'cancelled'), 2),\n",
              "  (('0', 'new', 'covid'), 2)],\n",
              " 'conspiracy': [(('new', 'world', 'order'), 29),\n",
              "  (('black', 'lives', 'matter'), 17),\n",
              "  (('million', 'x200b', 'x200b'), 14),\n",
              "  (('x200b', 'x200b', 'secretary'), 13),\n",
              "  (('military', 'intelligence', 'operation'), 10),\n",
              "  (('royal', 'death', 'racket'), 9),\n",
              "  (('one', 'world', 'government'), 9),\n",
              "  (('thousand', '6', 'months'), 8),\n",
              "  (('10', 'billion', 'trillion'), 8),\n",
              "  (('central', 'intelligence', 'agency'), 8)],\n",
              " 'criminalminds': [(('favourite', 'underrated', 'friendship'), 1),\n",
              "  (('underrated', 'friendship', 'part'), 1),\n",
              "  (('friendship', 'part', '2'), 1),\n",
              "  (('part', '2', 'view'), 1),\n",
              "  (('2', 'view', 'poll'), 1),\n",
              "  (('view', 'poll', '//www.reddit.com/poll/k2rntc'), 1),\n",
              "  (('poll', '//www.reddit.com/poll/k2rntc', 'elle'), 1),\n",
              "  (('//www.reddit.com/poll/k2rntc', 'elle', 'emily'), 1),\n",
              "  (('elle', 'emily', 'felt'), 1),\n",
              "  (('emily', 'felt', 'like'), 1)],\n",
              " 'donaldtrump': [(('donald', 'j.', 'trump'), 27),\n",
              "  (('j.', 'trump', '``'), 22),\n",
              "  (('republican', 'national', 'convention'), 12),\n",
              "  (('joe', 'biden', \"'s\"), 8),\n",
              "  (('national', 'convention', 'night'), 7),\n",
              "  (('2020', 'potus', 'schedule'), 7),\n",
              "  (('team', 'trump', '``'), 6),\n",
              "  (('president', 'trump', \"'s\"), 6),\n",
              "  (('president', 'donald', 'trump'), 6),\n",
              "  (('black', 'lives', 'matter'), 6)],\n",
              " 'gundeals': [(('ship', 'tax', 'az'), 3),\n",
              "  (('coupon', 'code', '``'), 3),\n",
              "  (('nfa', 'yhm', 'turbo'), 2),\n",
              "  (('free', 'shipping', 'acc'), 2),\n",
              "  (('ammo', 'sellier', 'bellot'), 2),\n",
              "  (('rock', 'island', 'armory'), 2),\n",
              "  (('sds', 'imports', '1911'), 2),\n",
              "  (('imports', '1911', 'duty'), 2),\n",
              "  (('pistol', 'grand', 'power'), 2),\n",
              "  (('grand', 'power', 'stribog'), 2)],\n",
              " 'intermittentfasting': [(('dr.', 'jason', 'fung'), 3),\n",
              "  (('intermittent', 'fasting', 'longevity'), 2),\n",
              "  (('removed', 'face', 'gains'), 2),\n",
              "  (('trouble', 'sleeping', '‚Äô'), 2),\n",
              "  (('seeing', 'physical', 'changes'), 2),\n",
              "  (('physical', 'changes', '‚Äô'), 2),\n",
              "  (('daily', 'intermittent', 'fasting'), 1),\n",
              "  (('intermittent', 'fasting', 'thread'), 1),\n",
              "  (('fasting', 'thread', 'share'), 1),\n",
              "  (('thread', 'share', 'daily'), 1)],\n",
              " 'l4d2': [(('new', 'players', 'like'), 1),\n",
              "  (('players', 'like', '‚Äô'), 1),\n",
              "  (('like', '‚Äô', 'learnding'), 1),\n",
              "  (('‚Äô', 'learnding', 'congratulations'), 1),\n",
              "  (('learnding', 'congratulations', 'u/-leblanc-'), 1),\n",
              "  (('congratulations', 'u/-leblanc-', '...'), 1),\n",
              "  (('u/-leblanc-', '...', 'every'), 1),\n",
              "  (('...', 'every', 'match'), 1),\n",
              "  (('every', 'match', 'bad'), 1),\n",
              "  (('match', 'bad', 'guy'), 1)],\n",
              " 'opensource': [(('lemmy', 'open', 'source'), 1),\n",
              "  (('open', 'source', 'decentralized'), 1),\n",
              "  (('source', 'decentralized', 'reddit'), 1),\n",
              "  (('decentralized', 'reddit', 'alternative'), 1),\n",
              "  (('reddit', 'alternative', 'release'), 1),\n",
              "  (('alternative', 'release', 'v0.6.0'), 1),\n",
              "  (('release', 'v0.6.0', 'avatars'), 1),\n",
              "  (('v0.6.0', 'avatars', 'email'), 1),\n",
              "  (('avatars', 'email', 'notifications'), 1),\n",
              "  (('email', 'notifications', 'whole'), 1)],\n",
              " 'playboicarti': [(('\\u200e', '\\u200e', '\\u200e'), 88),\n",
              "  (('whole', 'lotta', 'red'), 35),\n",
              "  (('‚Äô', 'gon', 'na'), 19),\n",
              "  (('bruh', 'really', 'happening'), 16),\n",
              "  (('gon', 'na', 'get'), 13),\n",
              "  (('really', 'happening', 'bruh'), 12),\n",
              "  (('happening', 'bruh', 'really'), 12),\n",
              "  (('playboi', 'carti', 'removed'), 10),\n",
              "  (('‚Äô', 'think', '‚Äô'), 10),\n",
              "  (('pi', '‚Äô', 'erre'), 9)],\n",
              " 'politics': [((\"'ll\", 'right', 'eventually'), 1),\n",
              "  (('right', 'eventually', 'trump'), 1),\n",
              "  (('eventually', 'trump', 'insists'), 1),\n",
              "  (('trump', 'insists', 'coronavirus'), 1),\n",
              "  (('insists', 'coronavirus', 'disappear'), 1),\n",
              "  (('coronavirus', 'disappear', 'citing'), 1),\n",
              "  (('disappear', 'citing', 'evidence'), 1),\n",
              "  (('citing', 'evidence', 'obama'), 1),\n",
              "  (('evidence', 'obama', 'says'), 1),\n",
              "  (('obama', 'says', 'democrat'), 1)],\n",
              " 'razer': [(('razer', 'sea-invitational', '2020'), 7),\n",
              "  (('sea-invitational', '2020', 'dota2'), 5),\n",
              "  (('hdmi', '1', 'dp'), 3),\n",
              "  (('day', '1', 'razer'), 3),\n",
              "  (('lower', 'bracket', 'finals'), 3),\n",
              "  (('899', 'razer', 'raptor'), 2),\n",
              "  (('1', 'hdmi', '1'), 2),\n",
              "  (('1', 'dp', '3.5mm'), 2),\n",
              "  (('dp', '3.5mm', 'audio'), 2),\n",
              "  (('3.5mm', 'audio', 'jack'), 2)],\n",
              " 'rutgers': [(('intro', 'financial', 'accounting'), 3),\n",
              "  (('writing', 'labor', 'studies'), 3),\n",
              "  (('labor', 'studies', 'employment'), 3),\n",
              "  (('studies', 'employment', 'relations'), 3),\n",
              "  (('minor', 'developmental', 'psychology'), 2),\n",
              "  (('intro', 'stats', 'business'), 2),\n",
              "  (('stats', 'business', 'intro'), 2),\n",
              "  (('business', 'intro', 'financial'), 2),\n",
              "  (('refund', 'term', 'bill'), 2),\n",
              "  (('know', 'easy', 'upper'), 2)],\n",
              " 'sony': [(('sony', 'ifa', '2020'), 3),\n",
              "  (('looking', 'forward', 'sony'), 2),\n",
              "  (('forward', 'sony', 'ifa'), 2),\n",
              "  (('ifa', '2020', 'press'), 1),\n",
              "  (('2020', 'press', 'conference'), 1),\n",
              "  (('press', 'conference', 'going'), 1),\n",
              "  (('conference', 'going', 'ahead'), 1),\n",
              "  (('going', 'ahead', '‚Äô'), 1),\n",
              "  (('ahead', '‚Äô', 'sony'), 1),\n",
              "  (('‚Äô', 'sony', 'modify'), 1)],\n",
              " 'sportsbook': [(('//www.reddit.com/r/sportsbook/search', 'q=title', '3a'),\n",
              "   28),\n",
              "  (('monthly', '//www.reddit.com/r/sportsbook/search', 'q=title'), 21),\n",
              "  (('removed', 'sports', 'predictions'), 9),\n",
              "  (('sportsbook', 'list', '//www.reddit.com/r/sportsbook/wiki/sportsbooks'),\n",
              "   7),\n",
              "  (('list', '//www.reddit.com/r/sportsbook/wiki/sportsbooks', '/r/sportsbook'),\n",
              "   7),\n",
              "  (('//www.reddit.com/r/sportsbook/wiki/sportsbooks', '/r/sportsbook', 'chat'),\n",
              "   7),\n",
              "  (('/r/sportsbook', 'chat', '//discordapp.com/invite/0z5fkengsblokq4s'), 7),\n",
              "  (('chat', '//discordapp.com/invite/0z5fkengsblokq4s', 'general'), 7),\n",
              "  (('//discordapp.com/invite/0z5fkengsblokq4s',\n",
              "    'general',\n",
              "    'discussion/questions'),\n",
              "   7),\n",
              "  (('general', 'discussion/questions', 'biweekly'), 7)],\n",
              " 'touhou': [(('touhou', 'music', 'thread'), 18),\n",
              "  (('doujin', 'mix', 'touhou'), 9),\n",
              "  (('index',\n",
              "    'thread',\n",
              "    '//old.reddit.com/r/touhou/comments/einuiz/touhou_music_thread_index/'),\n",
              "   7),\n",
              "  (('gensokyo', 'would', 'rather'), 6),\n",
              "  (('index',\n",
              "    'thread',\n",
              "    '//www.reddit.com/r/touhou/comments/gatvxg/touhou_music_thread_index_windows/'),\n",
              "   6),\n",
              "  (('daily', 'gensokyo', 'would'), 5),\n",
              "  (('ahead', 'lt', 'thought'), 5),\n",
              "  (('lt', 'thought', 'play'), 5),\n",
              "  (('thread',\n",
              "    '//old.reddit.com/r/touhou/comments/einuiz/touhou_music_thread_index/',\n",
              "    'touhou'),\n",
              "   5),\n",
              "  (('//old.reddit.com/r/touhou/comments/einuiz/touhou_music_thread_index/',\n",
              "    'touhou',\n",
              "    'music'),\n",
              "   5)],\n",
              " 'virginvschad': [(('chadcano', 'season', '4'), 3),\n",
              "  (('season', '4', 'episode'), 2),\n",
              "  (('names', 'thad', 'tracy'), 2),\n",
              "  (('would', 'like', 'announce'), 2),\n",
              "  (('official', 'vvc', 'height'), 1),\n",
              "  (('vvc', 'height', 'incel'), 1),\n",
              "  (('height', 'incel', 'bruce'), 1),\n",
              "  (('incel', 'bruce', '..'), 1),\n",
              "  (('bruce', '..', 'nonexistentad'), 1),\n",
              "  (('..', 'nonexistentad', 'behold'), 1)],\n",
              " 'wicked_edge': [(('full', 'show', 'online'), 6),\n",
              "  (('show', 'online', 'free'), 6),\n",
              "  (('cyril', 'r.', 'salter'), 5),\n",
              "  (('bulgari', 'man', 'black'), 3),\n",
              "  (('r.', 'salter', 'french'), 3),\n",
              "  (('salter', 'french', 'vetiver'), 3),\n",
              "  (('crs', 'french', 'vetiver'), 3),\n",
              "  (('english', 'subbed', 'watch'), 3),\n",
              "  (('sotd', 'sotn', 'sotd'), 3),\n",
              "  (('zingari', 'man', 'blacksmith'), 2)],\n",
              " 'worldbuilding': [(('little', 'lore', 'game'), 10),\n",
              "  (('lore', 'game', 'coming'), 10),\n",
              "  (('game', 'coming', 'small'), 10),\n",
              "  (('coming', 'small', 'things'), 10),\n",
              "  (('small', 'things', 'worlds'), 10),\n",
              "  (('things', 'worlds', 'challenges'), 10),\n",
              "  (('worlds', 'challenges', 'posted'), 10),\n",
              "  (('trying', 'get', 'something'), 10),\n",
              "  (('get', 'something', 'world'), 10),\n",
              "  (('something', 'world', 'nothing'), 10)],\n",
              " 'xqcow': [(('lego', 'harry', 'potter'), 10),\n",
              "  (('harry', 'potter', 'babyrage'), 10),\n",
              "  (('potter', 'babyrage', 'lego'), 9),\n",
              "  (('babyrage', 'lego', 'harry'), 9),\n",
              "  (('posting', 'widepeepohappy', 'xqc'), 8),\n",
              "  (('widepeepohappy', 'xqc', 'stares'), 6),\n",
              "  (('xqc', 'stares', 'camera'), 6),\n",
              "  (('stares', 'camera', 'says'), 6),\n",
              "  (('camera', 'says', 'widepeepohappy'), 6),\n",
              "  (('says', 'widepeepohappy', 'day'), 3)]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8E010UbQyML"
      },
      "source": [
        "## P1.2 - Answering questions with pandas\n",
        "\n",
        "Your task is to use pandas to answer questions about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmG2VIYQ93I"
      },
      "source": [
        "### P1.2.1 - Authors that post highly commented posts\n",
        "\n",
        "Find the top 1000 most commented posts. Then, obtain the names of the authors that have at least 3 posts among these posts.\n",
        "\n",
        "**What to implement:** Implement a function `find_popular_authors(df)` that takes as input the original dataframe and returns a list strings, where each string is the name of authors that satisfy the above criteria."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_popular_authors(df):\n",
        "\n",
        "  \"\"\"\n",
        "    This function takes a DataFrame with 'author' and 'num_comments' columns, filters the top 1000 authors with the \n",
        "    highest number of comments, and returns a list of the authors who have at least 3 comments in the top 1000.\n",
        "    \n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): A DataFrame containing 'author' and 'num_comments' columns.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of popular authors who have at least 3 comments in the top 1000.\n",
        "  \"\"\"\n",
        "  \n",
        "  df_filtered=df[['author','num_comments']]\n",
        "  df_sorted=df_filtered.sort_values(by='num_comments', ascending=False)\n",
        "  df_sorted_1000=df_sorted.head(1000)\n",
        "  author_counts=df_sorted_1000['author'].value_counts()\n",
        "  popular_authors=author_counts[author_counts>=3]\n",
        "  list_popular_authors=popular_authors.index.tolist()\n",
        "  return list_popular_authors"
      ],
      "metadata": {
        "id": "URKIW6oMvrYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_popular_authors(df)"
      ],
      "metadata": {
        "id": "tmsJyZ1_xpGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd82cd5-b803-499d-99b5-84e6aae3a6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AutoModerator',\n",
              " 'r[***]og',\n",
              " 'jigsawmap',\n",
              " 'Salramm01',\n",
              " 'HippolasCage',\n",
              " 'FunPeach0',\n",
              " 'iSlingShlong',\n",
              " 'Stoaticor',\n",
              " 'kevinmrr',\n",
              " 'ratioetlogicae',\n",
              " 'None',\n",
              " 'harushiga',\n",
              " 'tefunka',\n",
              " 'SlobBarker',\n",
              " 'stargem5',\n",
              " 'AristonD',\n",
              " 'werdmouf',\n",
              " 'Cross_Ange',\n",
              " 'samzz41',\n",
              " 'itsreallyreallytrue',\n",
              " 'SUPERGUESSOUS',\n",
              " 'Frocharocha',\n",
              " 'habichuelacondulce',\n",
              " 'CantStopPoppin',\n",
              " 'Allstarhit',\n",
              " 'theitguyforever',\n",
              " 'rebooted_life_42',\n",
              " 'Zhana-Aul',\n",
              " 'Not4Reel',\n",
              " 'Jellyrollrider',\n",
              " 'NYLaw',\n",
              " 'MakeItRainSheckels',\n",
              " 'TurtleFacts72',\n",
              " 'Defie-LOH-Gic',\n",
              " 'Typoqueen00',\n",
              " 'imagepoem',\n",
              " 'nycsellit4me',\n",
              " 'madman320',\n",
              " 'mythrowawaybabies',\n",
              " 'kogeliz',\n",
              " 'strngerdngermaus',\n",
              " 'Kinmuan',\n",
              " 'AllisonGator',\n",
              " 'Antiliani',\n",
              " 'vizard673',\n",
              " 'notpreposterous',\n",
              " 'BanDerUh',\n",
              " 'dukey',\n",
              " 'BebeFanMasterJ',\n",
              " 'Fr1sk3r',\n",
              " 'Gambit08',\n",
              " 'XDitto',\n",
              " 'elt0p0',\n",
              " 'twistedlogicx',\n",
              " 'TAKEitTOrCIRCLEJERK',\n",
              " 'Ramy_',\n",
              " 'tacolben',\n",
              " 'Morihando',\n",
              " '2020c[***]er[***]',\n",
              " 'dunphish64',\n",
              " 'apocalypticalley',\n",
              " 'dsbwayne',\n",
              " 'schuey_08',\n",
              " 'blacked_lover',\n",
              " 'stealthyfrog',\n",
              " 'TheFearlessWarrior',\n",
              " 'akarim5847',\n",
              " 'invertedparado[***]',\n",
              " 'faab64',\n",
              " 'bgny',\n",
              " 'ufgman',\n",
              " 'ReginaldJohnston',\n",
              " 'Singularitytracker',\n",
              " 'Lshim',\n",
              " 'chakalakasp',\n",
              " 'Romano16',\n",
              " 'foodforthinks',\n",
              " 'Mahomeboy_',\n",
              " 'lilmcfuggin',\n",
              " 'MisterT12',\n",
              " 'Majnum',\n",
              " 'CLO_Junkie',\n",
              " 'epiphanyx99',\n",
              " 'MrRoxx',\n",
              " 'Saibasaurus',\n",
              " 'KatieAllTheTime',\n",
              " 'boomerpro',\n",
              " 'le_br1t',\n",
              " 'hilltopye',\n",
              " 'Lost_Distribution546',\n",
              " 'PlenitudeOpulence',\n",
              " 'Wagamaga',\n",
              " 'OldFashionedJizz',\n",
              " 'WorkTomorrow',\n",
              " 'mouthofreason',\n",
              " 'hildebrand_rarity',\n",
              " 'DaFunkJunkie',\n",
              " 'SemperPereunt',\n",
              " 'Leg_holes',\n",
              " 'bemani4u',\n",
              " 'Playaguy',\n",
              " 'jollygreenscott91',\n",
              " 'allicat83',\n",
              " 'lanqian',\n",
              " 'into_the_[***]e',\n",
              " 'UpNDownCan',\n",
              " 'puppuli',\n",
              " 'johnruby',\n",
              " 'Madd-Nigrulo',\n",
              " 'OgranismAtWork',\n",
              " 'cdillon42',\n",
              " 'oliver_21',\n",
              " 'somnifacientsawyer',\n",
              " '[***]reader']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrl0kq09dxrp"
      },
      "source": [
        "### P1.2.2 - Distribution of posts per weekday\n",
        "\n",
        "Find the percentage of posts that were posted in each weekday (Monday, Tuesday, etc.). You can use an external calendar or you can use any functionality for dealing with dates available in pandas. \n",
        "\n",
        "**What to implement:** A function `get_weekday_post_distribution(df)` that takes as input the original dataframe and returns a dictionary of the form (the values are made up):\n",
        "\n",
        "```\n",
        "{'Monday': '14%',\n",
        "'Tuesday': '23%', \n",
        "...\n",
        "}\n",
        "```\n",
        "\n",
        "Note that you must only return two decimals, and you must include the percentage sign in the output dictionary. \n",
        "\n",
        "Note that in dictionaries order is not preserved, so the order in which it gets printed will not matter. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weekday_post_distribution(df):\n",
        "\n",
        "    \"\"\"\n",
        "    This function takes a pandas DataFrame with a 'posted_at' column that contains dates and times of posts. It creates a new 'weekday' column based on the weekday of each post, and then calculates the percentage of posts that were made on each day of the week. The result is returned as a dictionary where the keys are the names of the days of the week and the values are the percentages of posts made on each day.\n",
        "\n",
        "    Args:\n",
        "    - df: a pandas DataFrame with a 'posted_at' column containing dates and times of posts\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary where the keys are the names of the days of the week and the values are the percentages of posts made on each day\n",
        "    \"\"\"\n",
        "\n",
        "  # Convert 'posted_at' column to datetime format\n",
        "    df['posted_at'] = pd.to_datetime(df['posted_at'])\n",
        "\n",
        "    # Create 'weekday' column\n",
        "    df['weekday'] = df['posted_at'].dt.day_name()\n",
        "\n",
        "    # Count the number of posts for each weekday\n",
        "    counts = df['weekday'].value_counts()\n",
        "\n",
        "    # Calculate the percentage of posts for each weekday\n",
        "    percentages = (counts / counts.sum()) * 100\n",
        "    percentages = percentages.round(2).astype(str) + '%'\n",
        "\n",
        "    # Convert the result to a dictionary\n",
        "    result = percentages.to_dict()\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "Aj_2ss9jy9WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weekday_post_distribution(df)"
      ],
      "metadata": {
        "id": "5FUpOzgN1t3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40e124b-8b40-470b-b1e5-8efee085e4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Wednesday': '14.89%',\n",
              " 'Friday': '14.79%',\n",
              " 'Thursday': '14.75%',\n",
              " 'Tuesday': '14.54%',\n",
              " 'Monday': '14.31%',\n",
              " 'Saturday': '13.76%',\n",
              " 'Sunday': '12.96%'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj1WikSUPjO"
      },
      "source": [
        "### P1.2.3 - The 100 most passionate redditors\n",
        "\n",
        "We would like to know which are the 100 redditors (`author` column) that are most passionate. We will measure this by checking, for each redditor, the ratio at which they use adjectives. This ratio will be computed by dividing number of adjectives by the total number of words each redditor used. The analysis will only consider redditors that have written at least 1000 words.\n",
        "\n",
        "**What to implement:** A function called `get_passionate_redditors(df)` that takes as input the original dataframe and returns a list of the top 100 redditors (authors) by the ratio at which they use adjectives considering both the `title` and `selftext` columns. The returned list should be a list of tuples, where each inner tuple has two elements: the redditor (author) name, and the ratio of adjectives they used. The returned list should be sorted by adjective ratio in descending order (highest first). Only redditors that wrote more than 1000 words should be considered. You should use `nltk`'s `word_tokenize` and `pos_tag` functions to tokenize and find adjectives. You do not need to do any preprocessing like stopword removal, lemmatization or stemming."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_passionate_redditors(df):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns a list of top 100 redditors based on their ratio of adjectives to total words used in their posts.\n",
        "    \n",
        "    Args:\n",
        "    df (pandas.DataFrame): Dataframe containing Reddit post data with columns 'author', 'title', and 'selftext'.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of tuples containing the top 100 redditors and their corresponding adjective ratios in descending order.\n",
        "    Each tuple is of the form (author_name, adjective_ratio).\n",
        "    \"\"\"\n",
        "    \n",
        "  # Filter dataframe to only include redditors who have written at least 1000 words\n",
        "    df = df.groupby('author').filter(lambda x: len(' '.join(x['title']) + ' '.join(x['selftext'])) > 1000)\n",
        "    \n",
        "    # Tokenize the text for each redditor\n",
        "    redditor_tokens = {}\n",
        "    for author, group in df.groupby('author'):\n",
        "        text = ' '.join(group['title']) + ' '.join(group['selftext'])\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        redditor_tokens[author] = tokens\n",
        "        \n",
        "    # Count the number of adjectives for each redditor\n",
        "    redditor_adj_counts = {}\n",
        "    for author, tokens in redditor_tokens.items():\n",
        "        tagged_tokens = nltk.pos_tag(tokens)\n",
        "        adj_count = Counter(tag for word, tag in tagged_tokens if tag.startswith('JJ'))\n",
        "        redditor_adj_counts[author] = adj_count['JJ']\n",
        "        \n",
        "    # Calculate the adjective ratio for each redditor\n",
        "    redditor_ratios = {}\n",
        "    for author, tokens in redditor_tokens.items():\n",
        "        word_count = len(tokens)\n",
        "        adj_count = redditor_adj_counts[author]\n",
        "        ratio = adj_count / word_count\n",
        "        redditor_ratios[author] = ratio\n",
        "        \n",
        "    # Sort the redditors by their adjective ratio in descending order and return the top 100\n",
        "    sorted_redditors = sorted(redditor_ratios.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_100_redditors = sorted_redditors[:100]\n",
        "    \n",
        "    return top_100_redditors"
      ],
      "metadata": {
        "id": "yjfpDjS2oPzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_passionate_redditors(df)"
      ],
      "metadata": {
        "id": "7ddl-35Trg2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9f1f0c-a022-4b4a-c907-5a26f7053508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('OhanianIsTheBest', 0.1473768522226672),\n",
              " ('srvnmdomdotnet', 0.13358778625954199),\n",
              " ('madman320', 0.13114754098360656),\n",
              " ('healrstreettalk', 0.12574341546304163),\n",
              " ('wezafabregas', 0.12299465240641712),\n",
              " ('FreedomBoners', 0.12229976736457294),\n",
              " ('inspiration_capsule', 0.11650485436893204),\n",
              " ('bradipaurbana', 0.11538461538461539),\n",
              " ('Joe_Tazuna', 0.11428571428571428),\n",
              " ('Jumido730', 0.11323328785811733),\n",
              " ('factfind', 0.11072380561820791),\n",
              " ('mubukugrappa', 0.11038961038961038),\n",
              " ('DogMeatTalk', 0.10987261146496816),\n",
              " ('Playaguy', 0.10897435897435898),\n",
              " ('LisaMck041', 0.10880829015544041),\n",
              " ('TheAtheistArab87', 0.10759493670886076),\n",
              " ('superegz', 0.10588235294117647),\n",
              " ('asad1ali2', 0.1054421768707483),\n",
              " ('mushroomsarefriends', 0.10404624277456648),\n",
              " ('Travis-Cole', 0.10388580491673276),\n",
              " ('LanJiaoDuaKee', 0.10344827586206896),\n",
              " ('spiritofcom', 0.102880658436214),\n",
              " ('clme', 0.10222222222222223),\n",
              " ('ComradePetri', 0.10218978102189781),\n",
              " ('abdouh15', 0.1),\n",
              " ('goodnewsies', 0.1),\n",
              " ('seamslegit', 0.09987357774968394),\n",
              " ('linaching', 0.09895833333333333),\n",
              " ('CagedSilver', 0.09841269841269841),\n",
              " ('-ZeuS--', 0.09836065573770492),\n",
              " ('fullbloodedwhitemale', 0.09772951628825272),\n",
              " ('BalkanEagles', 0.09722222222222222),\n",
              " ('Molire', 0.0970873786407767),\n",
              " ('Vaye2013', 0.09665427509293681),\n",
              " ('tmull3n', 0.0962962962962963),\n",
              " ('SecretAgentIceBat', 0.09554413024850043),\n",
              " ('NDSB', 0.09547738693467336),\n",
              " ('I_am_a_[***]_to_ants', 0.09464285714285714),\n",
              " ('Jnuck82', 0.0943952802359882),\n",
              " ('lilshawnyy420', 0.09384775808133472),\n",
              " ('ValuableEquivalent2', 0.09382716049382717),\n",
              " ('Skoppe_', 0.09375),\n",
              " ('YoungSneakerheadXD', 0.09363295880149813),\n",
              " ('lanqian', 0.09340659340659341),\n",
              " ('backpackwayne', 0.09286293592862936),\n",
              " ('TelephoneSanitiser', 0.09273182957393483),\n",
              " ('stereomatch', 0.09259259259259259),\n",
              " ('Dismal_Structure', 0.09216589861751152),\n",
              " ('imoffabeanrn', 0.09191176470588236),\n",
              " ('CLO_Junkie', 0.09176470588235294),\n",
              " ('EMB1981', 0.09135135135135135),\n",
              " ('coolbern', 0.09090909090909091),\n",
              " ('Tripmooney', 0.09019947961838681),\n",
              " ('not_slim_shaddy', 0.09012875536480687),\n",
              " ('Ragnarokcometh', 0.09004739336492891),\n",
              " ('OnHolidayHere', 0.08949416342412451),\n",
              " ('sadtravisscott', 0.08934707903780069),\n",
              " ('zaaxuk', 0.08928571428571429),\n",
              " ('Nawaao', 0.08860759493670886),\n",
              " ('ubergrrrl', 0.08860759493670886),\n",
              " ('hennynow', 0.08847184986595175),\n",
              " ('bear-rah', 0.08833922261484099),\n",
              " ('CharlieXBravo', 0.0880952380952381),\n",
              " ('InferredVolatility', 0.0880503144654088),\n",
              " ('ginoenidok', 0.08796296296296297),\n",
              " ('Payt714', 0.08793103448275862),\n",
              " ('GeAlltidUpp', 0.08696660482374768),\n",
              " ('Venus230', 0.08649093904448106),\n",
              " ('raulsossa', 0.08626198083067092),\n",
              " ('tenders74', 0.08620689655172414),\n",
              " ('Ninten-Doh', 0.08557457212713937),\n",
              " ('orourkeee', 0.08518518518518518),\n",
              " ('nyello-2000', 0.085),\n",
              " ('PLUTO_HAS_COME_BACK', 0.08496732026143791),\n",
              " ('Humble_Award_4873', 0.08472012102874432),\n",
              " ('strngerdngermaus', 0.08455882352941177),\n",
              " ('greyuniwave', 0.08421052631578947),\n",
              " ('sbpotdbot', 0.08394366197183098),\n",
              " ('kent_k', 0.08391608391608392),\n",
              " ('nerdymen242424', 0.08385744234800839),\n",
              " ('MrClerkity', 0.08374384236453201),\n",
              " ('K-car-dial24', 0.08370044052863436),\n",
              " ('mission_improbables', 0.0833482702993368),\n",
              " ('abscbnnotforsale', 0.08333333333333333),\n",
              " ('icloudbug', 0.08333333333333333),\n",
              " ('th3allyK4t', 0.08333333333333333),\n",
              " ('35quai', 0.08328479906814211),\n",
              " ('Keppelin', 0.08274231678486997),\n",
              " ('leeyuiwah', 0.08264462809917356),\n",
              " ('theinfinitelight', 0.0825),\n",
              " ('RaoulDuke209', 0.08247422680412371),\n",
              " ('joshlreddit', 0.08237986270022883),\n",
              " ('rrixham', 0.08235844642021525),\n",
              " ('suckseggs', 0.08228980322003578),\n",
              " ('Ford456fgfd', 0.0821917808219178),\n",
              " ('Defie-LOH-Gic', 0.08206686930091185),\n",
              " ('sorryimveryhigh', 0.08200734394124846),\n",
              " ('Coleblade', 0.08168642951251646),\n",
              " ('FlatwormKey1400', 0.08163265306122448),\n",
              " ('cygnus489', 0.0815450643776824)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}